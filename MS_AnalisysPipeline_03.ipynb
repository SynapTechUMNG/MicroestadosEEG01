{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKixR6HLTB1TpV0MShayRV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SynapTechUMNG/MicroestadosEEG01/blob/main/MS_AnalisysPipeline_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenNeuro + BIDS en Drive es una ruta muy estándar. Abajo te dejo un notebook-style (celdas para Colab) para:\n",
        "\n",
        "Montar Google Drive\n",
        "\n",
        "Descargar el dataset de OpenNeuro dentro de Drive (usando datalad o openneuro-py)\n",
        "\n",
        "Verificar que es BIDS EEG/MEG válido\n",
        "\n",
        "Cargar con MNE y correr un pipeline de microestados (defaults razonables) con pycrostates"
      ],
      "metadata": {
        "id": "yD2-53MeQzmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0) Celda: Montar Drive**"
      ],
      "metadata": {
        "id": "cqqLqOKEKNtj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXpscJqPJ4Hw",
        "outputId": "fc982423-1207-4064-89fc-8d91fe3da15d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1) Celda CONFIG (EDITA AQUÍ)**"
      ],
      "metadata": {
        "id": "erZAbwwmK3PZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== EDITA AQUÍ ======\n",
        "DATA_ROOT = \"/content/drive/MyDrive/EEG_BIDS\"  # carpeta en Drive\n",
        "DS_ID = \"ds007006\"                             # <- tu dataset de OpenNeuro, p.ej. \"ds004000\"\n",
        "DOWNLOAD_METHOD = \"datalad\"                    # \"datalad\" recomendado; o \"openneuro\"\n",
        "\n",
        "# Microestados (defaults)\n",
        "K_MODE = \"fixed\"                               # \"fixed\" o \"sweep_4_8\"\n",
        "K_FIXED = 4                                    # si K_MODE=\"fixed\"\n",
        "GFP_MIN_PEAK_DISTANCE_SEC = 0.01               # 10 ms\n",
        "BPASS = (1., 40.)                              # Hz\n",
        "RESAMPLE = 250                                 # Hz (None para no resamplear)\n",
        "CROP_SEC = None                                # p.ej. 120 para pruebas rápidas; None = todo\n",
        "\n",
        "# Suavizado/segmentación (predict)\n",
        "SMOOTH_FACTOR = 10                             # 0 = sin suavizado (pycrostates predict)\n",
        "HALF_WINDOW_SIZE = 1\n",
        "MIN_SEGMENT_LENGTH_SEC = 0.03                  # 30 ms típico\n",
        "# =========================\n",
        "\n",
        "import pathlib, os\n",
        "DATA_ROOT = pathlib.Path(DATA_ROOT)\n",
        "DATASET_PATH = DATA_ROOT / DS_ID\n",
        "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "print(\"DATASET_PATH:\", DATASET_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTXaqaAbK9ej",
        "outputId": "788dd1b5-f996-4665-82c9-fe288222465f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATASET_PATH: /content/drive/MyDrive/EEG_BIDS/ds007006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2) Descargar desde OpenNeuro a Drive (elige una)**\n",
        "## **Opción A** (recomendada): datalad (rápida, eficiente; baja solo lo que pides)"
      ],
      "metadata": {
        "id": "iayAmf6BK942"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -qq update\n",
        "!apt-get -qq install -y git-annex\n",
        "!pip -q install datalad\n",
        "\n",
        "if not DATASET_PATH.exists():\n",
        "    !datalad install -s https://github.com/OpenNeuroDatasets/ds007006.git {DATASET_PATH}\n",
        "\n",
        "# Trae metadatos BIDS básicos\n",
        "!datalad -C {DATASET_PATH} get dataset_description.json participants.tsv 2>/dev/null || true\n",
        "\n",
        "# Trae EEG (ajusta patrones si lo necesitas)\n",
        "!datalad -C {DATASET_PATH} get -n\n",
        "!datalad -C {DATASET_PATH} get \"sub-*/**/*_eeg.*\" \"sub-*/**/*_channels.tsv\" \"sub-*/**/*_events.tsv\" 2>/dev/null || true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d8djwZLLRsV",
        "outputId": "ace1e689-14b8-4886-9acd-fd185e39c942"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package netbase.\n",
            "(Reading database ... 117540 files and directories currently installed.)\n",
            "Preparing to unpack .../0-netbase_6.3_all.deb ...\n",
            "Unpacking netbase (6.3) ...\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "Preparing to unpack .../1-libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../2-libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../3-aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Selecting previously unselected package git-annex.\n",
            "Preparing to unpack .../4-git-annex_8.20210223-2ubuntu2_amd64.deb ...\n",
            "Unpacking git-annex (8.20210223-2ubuntu2) ...\n",
            "Selecting previously unselected package git-remote-gcrypt.\n",
            "Preparing to unpack .../5-git-remote-gcrypt_1.4-1_all.deb ...\n",
            "Unpacking git-remote-gcrypt (1.4-1) ...\n",
            "Selecting previously unselected package nocache.\n",
            "Preparing to unpack .../6-nocache_1.1-1_amd64.deb ...\n",
            "Unpacking nocache (1.1-1) ...\n",
            "Setting up git-remote-gcrypt (1.4-1) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Setting up nocache (1.1-1) ...\n",
            "Setting up netbase (6.3) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Setting up git-annex (8.20210223-2ubuntu2) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.11) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.6/88.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIt is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n",
            "Cloning:   0% 0.00/2.00 [00:00<?, ? candidates/s]\n",
            "Enumerating: 0.00 Objects [00:00, ? Objects/s]\u001b[A\n",
            "                                              \u001b[A\n",
            "Counting:   0% 0.00/1.31k [00:00<?, ? Objects/s]\u001b[A\n",
            "                                                \u001b[A\n",
            "Compressing:   0% 0.00/536 [00:00<?, ? Objects/s]\u001b[A\n",
            "                                                 \u001b[A\n",
            "Receiving:   0% 0.00/1.31k [00:00<?, ? Objects/s]\u001b[A\n",
            "Receiving:  57% 749/1.31k [00:00<00:00, 7.44k Objects/s]\u001b[A\n",
            "                                                        \u001b[A\n",
            "Resolving:   0% 0.00/396 [00:00<?, ? Deltas/s]\u001b[A\n",
            "Resolving:   1% 4.00/396 [15:56<26:02:40, 239s/ Deltas]\u001b[A\n",
            "\u001b[1;1minstall\u001b[0m(\u001b[1;31merror\u001b[0m): /content/drive/MyDrive/EEG_BIDS/ds007006 (\u001b[1;35mdataset\u001b[0m) [No working git-annex installation of version >= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.. You have version 8.20210223]\u001b[1;31m [No working git-annex installation of version >= 10.20230126. Visit http://handbook.datalad.org/r.html?install for instructions on how to install DataLad and git-annex.. You have version 8.20210223]\u001b[0m\n",
            "It is highly recommended to configure Git before using DataLad. Set both 'user.name' and 'user.email' configuration variables.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/datalad\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datalad/cli/main.py\", line 131, in main\n",
            "    chpwd(path)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datalad/utils.py\", line 1771, in __init__\n",
            "    os.chdir(path)  # for grep people -- ok, to chdir here!\n",
            "    ^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/EEG_BIDS/ds007006'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tip: datalad get acepta patrones. Si tu dataset es muy grande, es mejor bajar solo resting-state o solo ciertos sujetos."
      ],
      "metadata": {
        "id": "q2uvxoG4LhNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Opción B:** openneuro-py (descarga “clásica”)"
      ],
      "metadata": {
        "id": "sAI7c_JTLsDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install openneuro-py\n",
        "!openneuro download --dataset {DS_ID} --target_dir {DATASET_PATH}"
      ],
      "metadata": {
        "id": "Ws7UenCkLzCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3) Celda “CHECK PATHS” (verifica BIDS y qué archivos hay)**"
      ],
      "metadata": {
        "id": "uXvm8_YML1f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, json\n",
        "\n",
        "assert (DATASET_PATH / \"dataset_description.json\").exists(), \"Falta dataset_description.json (¿descarga incompleta?).\"\n",
        "eeg_files = glob.glob(str(DATASET_PATH / \"sub-*/**/*_eeg.*\"), recursive=True)\n",
        "print(\"EEG files encontrados:\", len(eeg_files))\n",
        "for f in eeg_files[:20]:\n",
        "    print(\" -\", f)\n",
        "\n",
        "assert len(eeg_files) > 0, \"No veo archivos *_eeg.* dentro de sub-*/. Revisa si el dataset es iEEG/MEG o si falta descargar.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "iSq6v17mL9kF",
        "outputId": "13727ff7-47dd-4b50-dbc7-0c6a3854887f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Falta dataset_description.json (¿descarga incompleta?).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1280861905.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"dataset_description.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Falta dataset_description.json (¿descarga incompleta?).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0meeg_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"sub-*/**/*_eeg.*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EEG files encontrados:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meeg_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Falta dataset_description.json (¿descarga incompleta?)."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4) Cargar BIDS con MNE + preprocesado mínimo (EEG)**"
      ],
      "metadata": {
        "id": "UsGAsT5HMFnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install mne mne-bids pycrostates\n",
        "\n",
        "import mne\n",
        "from mne_bids import BIDSPath, get_entity_vals, read_raw_bids\n",
        "\n",
        "mne.set_log_level(\"WARNING\")\n",
        "\n",
        "subjects = sorted(get_entity_vals(DATASET_PATH, \"subject\"))\n",
        "assert subjects, \"No detecto sujetos en el BIDS.\"\n",
        "sub = subjects[0]\n",
        "\n",
        "tasks = sorted(get_entity_vals(DATASET_PATH, \"task\", subject=sub))\n",
        "# Heurística: si existe algo tipo rest, lo prioriza\n",
        "task = None\n",
        "if tasks:\n",
        "    rest_like = [t for t in tasks if \"rest\" in t.lower()]\n",
        "    task = rest_like[0] if rest_like else tasks[0]\n",
        "\n",
        "runs = sorted(get_entity_vals(DATASET_PATH, \"run\", subject=sub, task=task)) if task else []\n",
        "run = runs[0] if runs else None\n",
        "\n",
        "print(\"Usando:\", dict(subject=sub, task=task, run=run))\n",
        "\n",
        "bp = BIDSPath(root=DATASET_PATH, subject=sub, task=task, run=run)\n",
        "raw = read_raw_bids(bp, verbose=False)  # también intenta poblar bads/anotaciones desde sidecars si existen :contentReference[oaicite:2]{index=2}\n",
        "raw.load_data()\n",
        "\n",
        "# Nos quedamos con EEG (y solo EEG)\n",
        "raw.pick(\"eeg\")\n",
        "\n",
        "# (Opcional) recorte para pruebas\n",
        "if CROP_SEC is not None:\n",
        "    raw.crop(tmax=float(CROP_SEC))\n",
        "\n",
        "# Referencia promedio (muy estándar para microestados)\n",
        "raw.set_eeg_reference(\"average\", projection=False)\n",
        "\n",
        "# Filtro típico\n",
        "raw.filter(BPASS[0], BPASS[1], fir_design=\"firwin\")\n",
        "\n",
        "# (Opcional) resample para acelerar\n",
        "if RESAMPLE is not None:\n",
        "    raw.resample(RESAMPLE)\n",
        "\n",
        "# Si no hay posiciones de electrodos (para topomaps), intento estándar 10-20 SOLO para visualización\n",
        "mont = raw.get_montage()\n",
        "ch_pos = None if mont is None else mont.get_positions().get(\"ch_pos\", {})\n",
        "if (mont is None) or (ch_pos is None) or (len(ch_pos) == 0):\n",
        "    try:\n",
        "        raw.set_montage(\"standard_1020\", match_case=False, on_missing=\"ignore\")\n",
        "        print(\"Montage: standard_1020 (fallback para topomaps)\")\n",
        "    except Exception as e:\n",
        "        print(\"No pude setear montage (topomaps pueden fallar):\", repr(e))\n",
        "\n",
        "print(raw)"
      ],
      "metadata": {
        "id": "lNp8w1tFMInb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5) Microestados (picos GFP → ModKMeans → backfitting)**\n",
        "\n",
        "## **⚠️ Ojo: en pycrostates**, extract_gfp_peaks usa min_peak_distance en muestras, así que lo convierto desde segundos."
      ],
      "metadata": {
        "id": "LDB702vwMMCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pycrostates.preprocessing import extract_gfp_peaks\n",
        "from pycrostates.cluster import ModKMeans\n",
        "\n",
        "sfreq = raw.info[\"sfreq\"]\n",
        "min_peak_samp = max(1, int(round(GFP_MIN_PEAK_DISTANCE_SEC * sfreq)))\n",
        "min_seg_samp  = max(0, int(round(MIN_SEGMENT_LENGTH_SEC * sfreq)))\n",
        "\n",
        "# 1) GFP peaks (devuelve un ChData, ideal para pasar directo a fit) :contentReference[oaicite:4]{index=4}\n",
        "peaks = extract_gfp_peaks(raw, picks=\"eeg\", min_peak_distance=min_peak_samp, reject_by_annotation=True)\n",
        "print(\"GFP peaks extraídos:\", peaks.get_data().shape)\n",
        "\n",
        "def fit_and_segment(k: int):\n",
        "    cluster = ModKMeans(n_clusters=k, random_state=42, n_init=20, max_iter=300, tol=1e-6)\n",
        "    cluster.fit(peaks)  # fit acepta ChData :contentReference[oaicite:5]{index=5}\n",
        "\n",
        "    # 2) backfitting/segmentación sobre el raw (predict devuelve RawSegmentation) :contentReference[oaicite:6]{index=6}\n",
        "    seg = cluster.predict(\n",
        "        raw,\n",
        "        picks=\"eeg\",\n",
        "        factor=SMOOTH_FACTOR,\n",
        "        half_window_size=HALF_WINDOW_SIZE,\n",
        "        min_segment_length=min_seg_samp,\n",
        "        reject_edges=True,\n",
        "        reject_by_annotation=True,\n",
        "    )\n",
        "    return cluster, seg\n",
        "\n",
        "if K_MODE == \"fixed\":\n",
        "    cluster, seg = fit_and_segment(K_FIXED)\n",
        "    best_k = K_FIXED\n",
        "else:\n",
        "    # barrido simple k=4..8 y elegimos el mejor por GEV del clustering (rápido y práctico)\n",
        "    results = []\n",
        "    models = {}\n",
        "    for k in range(4, 9):\n",
        "        c, s = fit_and_segment(k)\n",
        "        gev = getattr(c, \"GEV_\", np.nan)  # atributo documentado :contentReference[oaicite:7]{index=7}\n",
        "        results.append({\"k\": k, \"GEV_cluster\": float(gev)})\n",
        "        models[k] = (c, s)\n",
        "    res_df = pd.DataFrame(results).sort_values(\"GEV_cluster\", ascending=False)\n",
        "    display(res_df)\n",
        "    best_k = int(res_df.iloc[0][\"k\"])\n",
        "    cluster, seg = models[best_k]\n",
        "\n",
        "print(\"k elegido:\", best_k)"
      ],
      "metadata": {
        "id": "kwXk9G9oMdzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6) Figuras + parámetros (métricas) y export a Drive**\n",
        "\n",
        "## En RawSegmentation, las métricas típicas salen con compute_parameters() (duración, ocurrencia, cobertura, etc.)"
      ],
      "metadata": {
        "id": "jYN_xMNlMlBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Figuras\n",
        "fig_dir = DATASET_PATH / \"derivatives\" / \"microstates_pycrostates\" / \"figures\"\n",
        "fig_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "cluster.plot(topomap_args=dict(contours=0))\n",
        "plt.tight_layout()\n",
        "plt.savefig(fig_dir / f\"microstates_maps_sub-{sub}_task-{task}_run-{run}_k-{best_k}.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "seg.plot(tmin=0, tmax=min(10, raw.times[-1]))\n",
        "plt.tight_layout()\n",
        "plt.savefig(fig_dir / f\"microstates_seg_sub-{sub}_task-{task}_run-{run}_k-{best_k}.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Parámetros (dict) :contentReference[oaicite:9]{index=9}\n",
        "params = seg.compute_parameters(norm_gfp=True, return_dist=False)\n",
        "\n",
        "# Transiciones (opcional)\n",
        "tm = seg.compute_transition_matrix(stat=\"probability\")\n",
        "\n",
        "out_dir = DATASET_PATH / \"derivatives\" / \"microstates_pycrostates\"\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Guardar parámetros\n",
        "df = pd.DataFrame([params])\n",
        "df.insert(0, \"subject\", sub)\n",
        "df.insert(1, \"task\", task)\n",
        "df.insert(2, \"run\", run)\n",
        "df.insert(3, \"k\", best_k)\n",
        "csv_path = out_dir / f\"microstates_params_sub-{sub}_task-{task}_run-{run}_k-{best_k}.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(\"Guardado:\", csv_path)\n",
        "\n",
        "# Guardar transición\n",
        "tm_path = out_dir / f\"microstates_transition_sub-{sub}_task-{task}_run-{run}_k-{best_k}.csv\"\n",
        "pd.DataFrame(tm).to_csv(tm_path, index=False)\n",
        "print(\"Guardado:\", tm_path)\n",
        "\n",
        "# Guardar modelo (centros) para reproducibilidad\n",
        "npz_path = out_dir / f\"microstates_model_sub-{sub}_task-{task}_run-{run}_k-{best_k}.npz\"\n",
        "np.savez(npz_path, cluster_centers=cluster.cluster_centers_, ch_names=raw.ch_names)\n",
        "print(\"Guardado:\", npz_path)"
      ],
      "metadata": {
        "id": "JE1GU8-cMn_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7) (Opcional) Correr “por todos los sujetos” y consolidar CSV**\n",
        "\n",
        "## Si quieres, pega esta celda y te genera un CSV “long” con parámetros de todos los sub-* (con la misma lógica de task/run por sujeto, tomando el primero/rest-like)."
      ],
      "metadata": {
        "id": "MzuwBqDFMvoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_rows = []\n",
        "\n",
        "for sub in subjects:\n",
        "    tasks = sorted(get_entity_vals(DATASET_PATH, \"task\", subject=sub))\n",
        "    task = None\n",
        "    if tasks:\n",
        "        rest_like = [t for t in tasks if \"rest\" in t.lower()]\n",
        "        task = rest_like[0] if rest_like else tasks[0]\n",
        "    runs = sorted(get_entity_vals(DATASET_PATH, \"run\", subject=sub, task=task)) if task else []\n",
        "    run = runs[0] if runs else None\n",
        "\n",
        "    bp = BIDSPath(root=DATASET_PATH, subject=sub, task=task, run=run)\n",
        "    raw = read_raw_bids(bp, verbose=False)\n",
        "    raw.load_data()\n",
        "    raw.pick(\"eeg\")\n",
        "\n",
        "    raw.set_eeg_reference(\"average\", projection=False)\n",
        "    raw.filter(BPASS[0], BPASS[1], fir_design=\"firwin\")\n",
        "    if RESAMPLE is not None:\n",
        "        raw.resample(RESAMPLE)\n",
        "\n",
        "    sfreq = raw.info[\"sfreq\"]\n",
        "    min_peak_samp = max(1, int(round(GFP_MIN_PEAK_DISTANCE_SEC * sfreq)))\n",
        "    min_seg_samp  = max(0, int(round(MIN_SEGMENT_LENGTH_SEC * sfreq)))\n",
        "\n",
        "    mont = raw.get_montage()\n",
        "    ch_pos = None if mont is None else mont.get_positions().get(\"ch_pos\", {})\n",
        "    if (mont is None) or (ch_pos is None) or (len(ch_pos) == 0):\n",
        "        try:\n",
        "            raw.set_montage(\"standard_1020\", match_case=False, on_missing=\"ignore\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    peaks = extract_gfp_peaks(raw, picks=\"eeg\", min_peak_distance=min_peak_samp, reject_by_annotation=True)\n",
        "\n",
        "    # k fijo o barrido rápido por GEV\n",
        "    if K_MODE == \"fixed\":\n",
        "        best_k = K_FIXED\n",
        "        cluster = ModKMeans(n_clusters=best_k, random_state=42, n_init=20, max_iter=300, tol=1e-6)\n",
        "        cluster.fit(peaks)\n",
        "    else:\n",
        "        best = (-np.inf, None)\n",
        "        best_cluster = None\n",
        "        for k in range(4, 9):\n",
        "            c = ModKMeans(n_clusters=k, random_state=42, n_init=20, max_iter=300, tol=1e-6)\n",
        "            c.fit(peaks)\n",
        "            gev = float(getattr(c, \"GEV_\", np.nan))\n",
        "            if gev > best[0]:\n",
        "                best = (gev, k)\n",
        "                best_cluster = c\n",
        "        best_k = best[1]\n",
        "        cluster = best_cluster\n",
        "\n",
        "    seg = cluster.predict(\n",
        "        raw, picks=\"eeg\",\n",
        "        factor=SMOOTH_FACTOR, half_window_size=HALF_WINDOW_SIZE,\n",
        "        min_segment_length=min_seg_samp,\n",
        "        reject_edges=True, reject_by_annotation=True\n",
        "    )\n",
        "    params = seg.compute_parameters(norm_gfp=True, return_dist=False)\n",
        "\n",
        "    row = {\"subject\": sub, \"task\": task, \"run\": run, \"k\": best_k, **params}\n",
        "    all_rows.append(row)\n",
        "\n",
        "group_df = pd.DataFrame(all_rows)\n",
        "group_csv = (DATASET_PATH / \"derivatives\" / \"microstates_pycrostates\" / \"microstates_params_ALL.csv\")\n",
        "group_df.to_csv(group_csv, index=False)\n",
        "print(\"Guardado:\", group_csv)\n",
        "group_df.head()"
      ],
      "metadata": {
        "id": "gZ_hZJAFQoSD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}